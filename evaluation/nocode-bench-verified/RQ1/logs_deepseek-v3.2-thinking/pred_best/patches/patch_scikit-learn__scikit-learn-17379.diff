diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py
index 6d02e92..c97ac41 100644
--- a/sklearn/isotonic.py
+++ b/sklearn/isotonic.py
@@ -224,8 +224,12 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
         self.out_of_bounds = out_of_bounds
 
     def _check_fit_data(self, X, y, sample_weight=None):
-        if len(X.shape) != 1:
-            raise ValueError("X should be a 1d array")
+        # Allow 2d array with 1 feature
+        if X.ndim == 2 and X.shape[1] == 1:
+            X = X.ravel()
+        elif X.ndim != 1:
+            raise ValueError("X should be a 1d array or a 2d array with 1 feature")
+        return X
 
     def _build_f(self, X, y):
         """Build the f_ interp1d function."""
@@ -246,7 +250,13 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
 
     def _build_y(self, X, y, sample_weight, trim_duplicates=True):
         """Build the y_ IsotonicRegression."""
-        self._check_fit_data(X, y, sample_weight)
+        # Note: X is already checked and potentially flattened by _check_fit_data in fit()
+        # For internal calls, ensure X is 1d
+        if X.ndim != 1:
+            if X.ndim == 2 and X.shape[1] == 1:
+                X = X.ravel()
+            else:
+                raise ValueError("X should be a 1d array or a 2d array with 1 feature")
 
         # Determine increasing if auto-determination requested
         if self.increasing == 'auto':
@@ -295,7 +305,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
 
         Parameters
         ----------
-        X : array-like of shape (n_samples,)
+        X : array-like of shape (n_samples,) or (n_samples, 1)
             Training data.
 
         y : array-like of shape (n_samples,)
@@ -320,6 +330,9 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
         y = check_array(y, dtype=X.dtype, **check_params)
         check_consistent_length(X, y, sample_weight)
 
+        # Check and potentially flatten X to 1d
+        X = self._check_fit_data(X, y, sample_weight)
+        
         # Transform y by running the isotonic regression algorithm and
         # transform X accordingly.
         X, y = self._build_y(X, y, sample_weight)
@@ -339,7 +352,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
 
         Parameters
         ----------
-        T : array-like of shape (n_samples,)
+        T : array-like of shape (n_samples,) or (n_samples, 1)
             Data to transform.
 
         Returns
@@ -355,8 +368,11 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
 
         T = check_array(T, dtype=dtype, ensure_2d=False)
 
-        if len(T.shape) != 1:
-            raise ValueError("Isotonic regression input should be a 1d array")
+        # Allow 2d array with 1 feature
+        if T.ndim == 2 and T.shape[1] == 1:
+            T = T.ravel()
+        elif T.ndim != 1:
+            raise ValueError("Isotonic regression input should be a 1d array or a 2d array with 1 feature")
 
         # Handle the out_of_bounds argument by clipping if needed
         if self.out_of_bounds not in ["raise", "nan", "clip"]:
@@ -379,7 +395,7 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
 
         Parameters
         ----------
-        T : array-like of shape (n_samples,)
+        T : array-like of shape (n_samples,) or (n_samples, 1)
             Data to transform.
 
         Returns
@@ -406,4 +422,4 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
             self._build_f(self.X_thresholds_, self.y_thresholds_)
 
     def _more_tags(self):
-        return {'X_types': ['1darray']}
+        return {'X_types': ['1darray']}
