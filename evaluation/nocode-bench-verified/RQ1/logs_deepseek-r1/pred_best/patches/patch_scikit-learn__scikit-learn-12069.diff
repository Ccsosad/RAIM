diff --git a/sklearn/decomposition/_kernel_pca.py b/sklearn/decomposition/_kernel_pca.py
index 6f0266a..132fbdc 100644
--- a/sklearn/decomposition/_kernel_pca.py
+++ b/sklearn/decomposition/_kernel_pca.py
@@ -8,7 +8,7 @@ from scipy import linalg
 from scipy.sparse.linalg import eigsh
 
 from ..utils._arpack import _init_arpack_v0
-from ..utils.extmath import svd_flip
+from ..utils.extmath import svd_flip, randomized_svd
 from ..utils.validation import check_is_fitted, _check_psd_eigenvalues
 from ..utils.deprecation import deprecated
 from ..exceptions import NotFittedError
@@ -59,10 +59,10 @@ class KernelPCA(TransformerMixin, BaseEstimator):
         Learn the inverse transform for non-precomputed kernels.
         (i.e. learn to find the pre-image of a point)
 
-    eigen_solver : {'auto', 'dense', 'arpack'}, default='auto'
+    eigen_solver : {'auto', 'dense', 'arpack', 'randomized'}, default='auto'
         Select eigensolver to use. If n_components is much less than
-        the number of training samples, arpack may be more efficient
-        than the dense eigensolver.
+        the number of training samples, randomized (or arpack) may be more
+        efficient than the dense eigensolver.
 
     tol : float, default=0
         Convergence tolerance for arpack.
@@ -141,13 +141,23 @@ class KernelPCA(TransformerMixin, BaseEstimator):
         and Klaus-Robert Mueller. 1999. Kernel principal
         component analysis. In Advances in kernel methods,
         MIT Press, Cambridge, MA, USA 327-352.
+
+    For the randomized solver:
+        - Algorithm 4.3 in
+          "Finding structure with randomness: Stochastic algorithms for
+          constructing approximate matrix decompositions"
+          Halko, et al., 2009 (arXiv:0909.4061)
+        - "An implementation of a randomized algorithm for principal component
+          analysis"
+          A. Szlam et al. 2014 (arXiv:1412.3510)
     """
     @_deprecate_positional_args
     def __init__(self, n_components=None, *, kernel="linear",
                  gamma=None, degree=3, coef0=1, kernel_params=None,
                  alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',
                  tol=0, max_iter=None, remove_zero_eig=False,
-                 random_state=None, copy_X=True, n_jobs=None):
+                 random_state=None, copy_X=True, n_jobs=None,
+                 n_iter='auto', power_iteration_normalizer='auto'):
         if fit_inverse_transform and kernel == 'precomputed':
             raise ValueError(
                 "Cannot fit_inverse_transform with a precomputed kernel.")
@@ -166,6 +176,8 @@ class KernelPCA(TransformerMixin, BaseEstimator):
         self.random_state = random_state
         self.n_jobs = n_jobs
         self.copy_X = copy_X
+        self.n_iter = n_iter
+        self.power_iteration_normalizer = power_iteration_normalizer
 
     # TODO: Remove in 1.1
     # mypy error: Decorated property not supported
@@ -215,6 +227,30 @@ class KernelPCA(TransformerMixin, BaseEstimator):
                                                 tol=self.tol,
                                                 maxiter=self.max_iter,
                                                 v0=v0)
+        elif eigen_solver == 'randomized':
+            # Randomized SVD
+            n_oversamples = 10  # default as in randomized_svd
+            n_random = n_components + n_oversamples
+            if n_random > K.shape[0]:
+                n_random = K.shape[0]
+            
+            # Note: randomized_svd returns U, Sigma, VT where K ~= U * Sigma * VT
+            # But since K is symmetric, we use V as eigenvectors
+            U, Sigma, _ = randomized_svd(
+                K, n_components=n_random,
+                n_iter=self.n_iter,
+                power_iteration_normalizer=self.power_iteration_normalizer,
+                random_state=self.random_state,
+                flip_sign=False
+            )
+            
+            # The eigenvalues are the singular values of K since K is positive semi-definite
+            self.lambdas_ = Sigma
+            self.alphas_ = U
+            
+            # Select top n_components
+            self.lambdas_ = self.lambdas_[:n_components]
+            self.alphas_ = self.alphas_[:, :n_components]
 
         # make sure that the eigenvalues are ok and fix numerical issues
         self.lambdas_ = _check_psd_eigenvalues(self.lambdas_,
@@ -388,4 +424,4 @@ class KernelPCA(TransformerMixin, BaseEstimator):
 
     def _more_tags(self):
         return {'preserves_dtype': [np.float64, np.float32],
-                'pairwise': self.kernel == 'precomputed'}
+                'pairwise': self.kernel == 'precomputed'}
