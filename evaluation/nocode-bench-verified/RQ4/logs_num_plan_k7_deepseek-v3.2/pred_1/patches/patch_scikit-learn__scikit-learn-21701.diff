diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index d06dfcd..6530243 100644
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -302,12 +302,19 @@ class BaseRandomProjection(
 
     @abstractmethod
     def __init__(
-        self, n_components="auto", *, eps=0.1, dense_output=False, random_state=None
+        self,
+        n_components="auto",
+        *,
+        eps=0.1,
+        dense_output=False,
+        random_state=None,
+        compute_inverse_components=False,
     ):
         self.n_components = n_components
         self.eps = eps
         self.dense_output = dense_output
         self.random_state = random_state
+        self.compute_inverse_components = compute_inverse_components
 
     @abstractmethod
     def _make_random_matrix(self, n_components, n_features):
@@ -399,6 +406,15 @@ class BaseRandomProjection(
             " not the proper shape."
         )
 
+        # Compute inverse components if requested
+        if self.compute_inverse_components:
+            # Compute the pseudo-inverse of the components matrix
+            # Note: components_ is (n_components, n_features)
+            # We want inverse_components_ to be (n_features, n_components)
+            self.inverse_components_ = np.linalg.pinv(self.components_.T)
+        else:
+            self.inverse_components_ = None
+
         return self
 
     def transform(self, X):
@@ -429,6 +445,159 @@ class BaseRandomProjection(
         X_new = safe_sparse_dot(X, self.components_.T, dense_output=self.dense_output)
         return X_new
 
+    def inverse_transform(self, X):
+        """Project data back to its original space.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Data to be transformed back.
+
+        Returns
+        -------
+        X_original : ndarray of shape (n_samples, n_features)
+            Data in the original space. Always a dense array.
+        """
+        from sklearn.utils.validation import check_array
+
+        check_is_fitted(self)
+        X = check_array(X, dtype=[np.float64, np.float32])
+
+        if X.shape[1] != self.n_components_:
+            raise ValueError(
+                "The number of features in X must match the number of "
+                "components in the transformation. "
+                f"Expected {self.n_components_} features, got {X.shape[1]}."
+            )
+
+        if self.inverse_components_ is None:
+            inv_comp = np.linalg.pinv(self.components_.T)
+        else:
+            inv_comp = self.inverse_components_
+
+        # X @ inv_comp.T is the same as (inv_comp @ X.T).T, but we do the former.
+        return X @ inv_comp.T
+
+    def inverse_transform(self, X):
+        """Project data back to its original space.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Data to be transformed back.
+
+        Returns
+        -------
+        X_original : ndarray of shape (n_samples, n_features)
+            Data in the original space. Always a dense array.
+        """
+        check_is_fitted(self)
+        X = check_array(X, dtype=[np.float64, np.float32])
+
+        if X.shape[1] != self.n_components_:
+            raise ValueError(
+                "The number of features in X must match the number of "
+                "components in the transformation. "
+                f"Expected {self.n_components_} features, got {X.shape[1]}."
+            )
+
+        # If inverse_components_ is not precomputed, compute it now
+        if self.inverse_components_ is None:
+            # Compute the pseudo-inverse on the fly
+            inv_comp = np.linalg.pinv(self.components_.T)
+        else:
+            inv_comp = self.inverse_components_
+
+        # The inverse transform is X @ inv_comp.T
+        # But note: inv_comp is (n_features, n_components)
+        # So we do X @ inv_comp.T = X @ (n_components, n_features) ?
+        # Actually, we want to compute X @ inv_comp.T
+        # However, the documentation says: "it computes the product of the
+        # input X and the transpose of the inverse components."
+        # So if inverse_components_ is (n_features, n_components), then
+        # its transpose is (n_components, n_features).
+        # Therefore, we should do X @ inv_comp.T.
+        # But note: the documentation says "product of the input X and the transpose"
+        # which is exactly X @ inv_comp.T.
+        # However, the shape of X is (n_samples, n_components) and inv_comp.T is (n_components, n_features)
+        # So the result is (n_samples, n_features).
+
+        # Alternatively, we can do X @ inv_comp.T
+        # But note: the pseudo-inverse we computed is for components_.T?
+        # Actually, we stored the pseudo-inverse of components_.T as inverse_components_.
+        # So we don't need to transpose again? Let's clarify:
+        # We have components_ (n_components, n_features)
+        # We want to solve for X_original in: X = X_original @ components_.T
+        # => X_original = X @ pinv(components_.T)
+        # So pinv(components_.T) is (n_features, n_components) -> that's inverse_components_.
+        # Therefore, X_original = X @ inverse_components_.T? Wait:
+        # X_original = X @ pinv(components_.T) = X @ inverse_components_
+        # because inverse_components_ is pinv(components_.T).
+        # So we should do X @ inverse_components_ (without transpose).
+        # But the documentation says "product of the input X and the transpose of the inverse components"
+        # which would be X @ inverse_components_.T.
+        # There is a contradiction.
+
+        # Let's derive mathematically:
+        # We have: X_projected = X_original @ components_.T
+        # We want to get back X_original from X_projected.
+        # The least squares solution is: X_original = X_projected @ pinv(components_.T)
+        # Let M = components_.T (n_features, n_components)
+        # Then X_projected = X_original @ M.T
+        # The least squares solution for X_original is X_projected @ pinv(M.T)
+        # But note: pinv(M.T) = (M.T)^+ = (components_)^+? Actually, M = components_.T, so M.T = components_.
+        # So we need pinv(components_)?
+        # Alternatively, we can think of:
+        # X_projected = X_original @ components_.T
+        # Multiply both sides on the right by components_:
+        # X_projected @ components_ = X_original @ components_.T @ components_
+        # But that doesn't help.
+
+        # Actually, the standard linear algebra:
+        # If we have a system: Y = X @ A, then X = Y @ A^+ where A^+ is the pseudo-inverse of A.
+        # Here, A = components_.T (n_features, n_components)
+        # So X_original = X_projected @ (components_.T)^+
+        # So we should compute the pseudo-inverse of components_.T, which is (n_components, n_features)^T? 
+        # Actually, the pseudo-inverse of a matrix B is (B^T B)^{-1} B^T.
+        # But we don't need to derive. The documentation says:
+        # "it computes the product of the input X and the transpose of the inverse components."
+        # So we should follow the documentation.
+
+        # However, the documentation might be wrong? Let's look at the example in the requirements:
+        # They compute X_new_inversed = transformer.inverse_transform(X_new)
+        # and then X_new_again = transformer.transform(X_new_inversed)
+        # and they get the same as X_new.
+        # So we should implement what makes that work.
+
+        # Let's test with a small example in our head:
+        # Suppose components_ is a random matrix (n_components, n_features).
+        # We compute inverse_components_ as pinv(components_.T) [as we did above].
+        # Then inverse_transform(X) = X @ inverse_components_.T
+        # Then transform that result: (X @ inverse_components_.T) @ components_.T
+        # = X @ (inverse_components_.T @ components_.T) = X @ (components_ @ inverse_components_)^T
+        # But note: inverse_components_ is pinv(components_.T), so components_.T @ inverse_components_ = I (n_components, n_components)
+        # Therefore, inverse_components_.T @ components_.T = (components_ @ inverse_components_)^T = I^T = I.
+        # So we get back X. So it works.
+
+        # Therefore, we should do:
+        # X_original = X @ inverse_components_.T
+        # But note: we stored inverse_components_ as pinv(components_.T), which is (n_features, n_components)
+        # So inverse_components_.T is (n_components, n_features).
+        # So we do X @ inverse_components_.T.
+
+        # Alternatively, we can store the transpose directly? But the documentation says
+        # inverse_components_ is (n_features, n_components). So we follow that.
+
+        # So the code should be:
+        return X @ inv_comp.T
+        # But wait: if we computed inv_comp as pinv(components_.T), then it is (n_features, n_components)
+        # So we need to transpose it to get (n_components, n_features) for the multiplication.
+
+        # However, the documentation says: "it computes the product of the input X and the transpose of the inverse components."
+        # So we are doing exactly that: X @ (inv_comp.T)
+
+        # So let's implement that.
+
     @property
     def _n_features_out(self):
         """Number of transformed output features.
@@ -452,6 +621,97 @@ class GaussianRandomProjection(BaseRandomProjection):
 
     .. versionadded:: 0.13
 
+    Parameters
+    ----------
+    n_components : int or 'auto', default='auto'
+        Dimensionality of the target projection space.
+
+        n_components can be automatically adjusted according to the
+        number of samples in the dataset and the bound given by the
+        Johnson-Lindenstrauss lemma. In that case the quality of the
+        embedding is controlled by the ``eps`` parameter.
+
+        It should be noted that Johnson-Lindenstrauss lemma can yield
+        very conservative estimated of the required number of components
+        as it makes no assumption on the structure of the dataset.
+
+    eps : float, default=0.1
+        Parameter to control the quality of the embedding according to
+        the Johnson-Lindenstrauss lemma when `n_components` is set to
+        'auto'. The value should be strictly positive.
+
+        Smaller values lead to better embedding and higher number of
+        dimensions (n_components) in the target projection space.
+
+    dense_output : bool, default=False
+        If True, ensure that the output of the random projection is a
+        dense numpy array even if the input is sparse. In practice,
+        if the number of components is small the number of zero
+        components in the projected data will be very small and will
+        require as much memory as the dense representation. If dense_output
+        is False, the output is a sparse matrix if the input is sparse.
+
+    random_state : int, RandomState instance or None, default=None
+        Controls the pseudo random number generator used to generate the
+        projection matrix at fit time.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    compute_inverse_components : bool, default=False
+        If True, compute the pseudo-inverse of the components matrix during
+        fit and store it as `inverse_components_`. This enables the
+        `inverse_transform` method to project data back to the original
+        space without recomputing the pseudo-inverse each time. If False,
+        the pseudo-inverse is computed on the fly when `inverse_transform`
+        is called, which can be costly for large matrices.
+
+        .. versionadded:: 1.1
+
+    Attributes
+    ----------
+    n_components_ : int
+        Concrete number of components computed when n_components="auto".
+
+    components_ : ndarray of shape (n_components, n_features)
+        Random matrix used for the projection.
+
+    inverse_components_ : ndarray of shape (n_features, n_components) or None
+        Pseudo-inverse of the components matrix. Only available if
+        `compute_inverse_components` is True.
+
+        .. versionadded:: 1.1
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.random_projection import GaussianRandomProjection
+    >>> rng = np.random.RandomState(42)
+    >>> X = rng.rand(100, 10000)
+    >>> transformer = GaussianRandomProjection(random_state=rng)
+    >>> X_new = transformer.fit_transform(X)
+    >>> X_new.shape
+    (100, 3947)
+
+    See Also
+    --------
+    SparseRandomProjection : Reduce dimensionality through sparse
+        random projection.
+
+    References
+    ----------
+    .. [1] Sanjoy Dasgupta. 2000. Experiments with random projection.
+           In Proceedings of the Sixteenth conference on Uncertainty in
+           artificial intelligence (UAI'00), Craig Boutilier and Mois√©s Goldszmidt
+           (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,
+           143-151.
+
+    .. [2] Ella Bingham and Heikki Mannila. 2001. Random projection in
+           dimensionality reduction: applications to image and text data.
+           In Proceedings of the seventh ACM SIGKDD international conference
+           on Knowledge discovery and data mining (KDD '01). ACM, New York,
+           NY, USA, 245-250.
+    """
+
     Parameters
     ----------
     n_components : int or 'auto', default='auto'
