diff --git a/sklearn/random_projection.py b/sklearn/random_projection.py
index d06dfcd..268ef53 100644
--- a/sklearn/random_projection.py
+++ b/sklearn/random_projection.py
@@ -302,11 +302,18 @@ class BaseRandomProjection(
 
     @abstractmethod
     def __init__(
-        self, n_components="auto", *, eps=0.1, dense_output=False, random_state=None
+        self,
+        n_components="auto",
+        *,
+        eps=0.1,
+        dense_output=False,
+        compute_inverse_components=False,
+        random_state=None,
     ):
         self.n_components = n_components
         self.eps = eps
         self.dense_output = dense_output
+        self.compute_inverse_components = compute_inverse_components
         self.random_state = random_state
 
     @abstractmethod
@@ -399,6 +406,15 @@ class BaseRandomProjection(
             " not the proper shape."
         )
 
+        # Compute inverse components if requested
+        if self.compute_inverse_components:
+            # Compute the pseudo-inverse of the components matrix
+            # The components matrix is of shape (n_components, n_features)
+            # The pseudo-inverse is of shape (n_features, n_components)
+            self.inverse_components_ = np.linalg.pinv(self.components_)
+        else:
+            self.inverse_components_ = None
+
         return self
 
     def transform(self, X):
@@ -429,6 +445,42 @@ class BaseRandomProjection(
         X_new = safe_sparse_dot(X, self.components_.T, dense_output=self.dense_output)
         return X_new
 
+    def inverse_transform(self, X):
+        """Project data back to its original space.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Data to be transformed back.
+
+        Returns
+        -------
+        X_original : ndarray of shape (n_samples, n_features)
+            Data in the original space. It will always be a dense array.
+        """
+        check_is_fitted(self)
+        X = self._validate_data(
+            X, accept_sparse=["csr", "csc"], reset=False, dtype=[np.float64, np.float32]
+        )
+
+        if X.shape[1] != self.n_components_:
+            raise ValueError(
+                "Impossible to perform inverse transform:"
+                "X has a different number of components than the transformer. "
+                "(%s != %s)" % (X.shape[1], self.n_components_)
+            )
+
+        # If inverse_components_ is not precomputed, compute it on the fly
+        if self.inverse_components_ is None:
+            inv_comp = np.linalg.pinv(self.components_)
+        else:
+            inv_comp = self.inverse_components_
+
+        # The inverse transform is X @ inv_comp.T
+        # We use dense_output=True to always return a dense array
+        X_original = safe_sparse_dot(X, inv_comp.T, dense_output=True)
+        return X_original
+
     @property
     def _n_features_out(self):
         """Number of transformed output features.
@@ -437,6 +489,162 @@ class BaseRandomProjection(
         """
         return self.n_components
 
+    def inverse_transform(self, X):
+        """Project data back to its original space.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Data to be transformed back.
+
+        Returns
+        -------
+        X_original : ndarray of shape (n_samples, n_features)
+            Data in the original space. It will always be a dense array.
+        """
+        check_is_fitted(self)
+        X = self._validate_data(
+            X, accept_sparse=["csr", "csc"], reset=False, dtype=[np.float64, np.float32]
+        )
+
+        if X.shape[1] != self.n_components_:
+            raise ValueError(
+                "Impossible to perform inverse transform:"
+                "X has a different number of components than the transformer. "
+                "(%s != %s)" % (X.shape[1], self.n_components_)
+            )
+
+        # If inverse_components_ is not precomputed, compute it on the fly
+        if self.inverse_components_ is None:
+            inv_comp = np.linalg.pinv(self.components_.T)
+        else:
+            inv_comp = self.inverse_components_
+
+        # The inverse transform is X @ inv_comp.T
+        # But note: inv_comp is of shape (n_features, n_components)
+        # So we need to do X @ inv_comp.T = X @ components_? Actually, no.
+        # We want: X_original = X @ inv_comp.T
+        # But the documentation says: "it computes the product of the input X and the transpose of the inverse components."
+        # So if inverse_components_ is (n_features, n_components), then its transpose is (n_components, n_features).
+        # Therefore, we should do X @ inv_comp.T.
+        # However, note that the pseudo-inverse of components_.T is computed as:
+        #   inv_comp = np.linalg.pinv(self.components_.T)
+        # So inv_comp is already the pseudo-inverse of components_.T.
+        # Therefore, the inverse transform is X @ inv_comp.T.
+        # But let's double-check: we want to approximate X_original such that:
+        #   X = X_original @ components_.T
+        # So X_original = X @ (components_.T)^+ = X @ inv_comp.
+        # Wait, no: if we define inv_comp = (components_.T)^+, then X_original = X @ inv_comp.
+        # But we computed inv_comp as the pseudo-inverse of components_.T, so it is exactly (components_.T)^+.
+        # So we should do X @ inv_comp.
+        # However, the documentation says: "it computes the product of the input X and the transpose of the inverse components."
+        # So there is a transpose. Let's read the documentation again:
+        # "When the inverse_transform method is called, it computes the product of the input X and the transpose of the inverse components."
+        # So if inverse_components_ is (n_features, n_components), then its transpose is (n_components, n_features).
+        # Then the product X (n_samples, n_components) @ transpose(inverse_components) (n_components, n_features) -> (n_samples, n_features).
+        # But mathematically, we want X @ (components_.T)^+.
+        # So if inverse_components_ = (components_.T)^+, then we want X @ inverse_components_.
+        # So why the transpose? Actually, the documentation says "transpose of the inverse components", which is (inverse_components).T.
+        # So if we store inverse_components_ as (components_.T)^+, then we should return X @ inverse_components_.T.
+        # But that would be X @ ((components_.T)^+).T.
+        # That doesn't seem right.
+        # Let's derive:
+        # We have: X_projected = X_original @ components_.T
+        # Therefore, X_original = X_projected @ (components_.T)^+ = X_projected @ inv_comp.
+        # So we should return X @ inv_comp.
+        # So we should not transpose.
+        # But the documentation says "transpose of the inverse components". So maybe we are storing the inverse components differently?
+        # Actually, the documentation says: "inverse_components_ matrix has shape (n_features, n_components)".
+        # And we are storing the pseudo-inverse of components_.T, which is exactly (n_features, n_components).
+        # So inverse_components_ = (components_.T)^+.
+        # Then the inverse transform is X @ inverse_components_ (without transpose).
+        # So the documentation is wrong? Or we are misreading.
+        # Let's look at the documentation again: "it computes the product of the input X and the transpose of the inverse components."
+        # So if we follow the documentation, we should do X @ inverse_components_.T.
+        # But that would be X @ ((components_.T)^+).T.
+        # That is not correct.
+        # Alternatively, maybe we are storing the inverse components as the pseudo-inverse of components_ (not components_.T)?
+        # Then inverse_components_ would be (n_components, n_features)? But the documentation says (n_features, n_components).
+        # So we are in a contradiction.
+        # Let's look at the example in the documentation: they use SparseRandomProjection and then inverse_transform returns the original shape.
+        # We should test with a small example to see what is correct.
+        # But for now, let's follow the documentation and do X @ inverse_components_.T.
+        # However, note that in the fit we computed the pseudo-inverse of components_.T, which is (n_features, n_components).
+        # So if we do X @ inverse_components_.T, we get (n_samples, n_features) as desired.
+        # Actually, that might be the same as X @ (components_.T)^+? No.
+        # Let's do a small test in our head:
+        #   components_.T is (n_features, n_components)
+        #   inv_comp = pinv(components_.T) is (n_components, n_features)?? Wait, no.
+        # The pseudo-inverse of a matrix A (m,n) is (n,m). So if A = components_.T is (n_features, n_components), then pinv(A) is (n_components, n_features).
+        # So we are storing it as (n_components, n_features)? But the documentation says (n_features, n_components).
+        # So we are storing the transpose of the pseudo-inverse?
+        # Actually, we want to store the inverse components as (n_features, n_components). So we should store the pseudo-inverse of components_ (not components_.T).
+        # Because components_ is (n_components, n_features), so its pseudo-inverse is (n_features, n_components).
+        # That makes sense! So we should compute:
+        #   inverse_components_ = np.linalg.pinv(self.components_)
+        # Then the inverse transform is X @ inverse_components_.T.
+        # Because inverse_components_ is (n_features, n_components), so its transpose is (n_components, n_features).
+        # Then X (n_samples, n_components) @ inverse_components_.T (n_components, n_features) -> (n_samples, n_features).
+        # And mathematically, we have:
+        #   X_projected = X_original @ components_.T
+        # So X_original = X_projected @ (components_.T)^+ = X_projected @ pinv(components_.T)
+        # But note: pinv(components_.T) = (components_.T)^+ = (components_^+).T
+        # So if we let inverse_components_ = pinv(components_) = components_^+ (which is (n_features, n_components)), then:
+        #   inverse_components_.T = (components_^+).T = (components_.T)^+
+        # So indeed, X_original = X_projected @ inverse_components_.T.
+        # Therefore, we should compute the pseudo-inverse of components_ (not components_.T) and store it as inverse_components_.
+        # Then in inverse_transform, we do X @ inverse_components_.T.
+        # So let's change the fit accordingly.
+
+        # We'll change the fit to compute the pseudo-inverse of components_ (not components_.T).
+        # So in fit, we do:
+        #   self.inverse_components_ = np.linalg.pinv(self.components_)
+        # Then in inverse_transform, we do:
+        #   X_original = X @ self.inverse_components_.T
+
+        # So let's update the fit and inverse_transform accordingly.
+
+        # But note: we already wrote the fit change above. We need to change it to pinv(components_) instead of pinv(components_.T).
+
+        # So let's update the fit change again.
+
+        # Actually, we haven't written the inverse_transform yet. So we can write it correctly.
+
+        # So in fit:
+        #   self.inverse_components_ = np.linalg.pinv(self.components_)
+
+        # And in inverse_transform:
+        #   inv_comp = self.inverse_components_ if self.inverse_components_ is not None else np.linalg.pinv(self.components_)
+        #   X_original = X @ inv_comp.T
+
+        # But note: the result should be dense.
+        # So we should convert X to dense if it is sparse? Actually, the documentation says the result is always dense.
+        # So we can do:
+
+        #   X_original = X @ inv_comp.T
+
+        # But if X is sparse, the product might be sparse? We want dense. So we can use safe_sparse_dot with dense_output=True.
+
+        # So let's write the inverse_transform.
+
+        # First, let's update the fit to compute the pseudo-inverse of components_:
+
+        # We'll change the fit block we wrote above.
+
+        # Then write the inverse_transform.
+
+        # So here is the inverse_transform method.
+
+        # We'll use safe_sparse_dot to handle sparse X and force dense output.
+
+        inv_comp = self.inverse_components_
+        if inv_comp is None:
+            inv_comp = np.linalg.pinv(self.components_)
+
+        # The inverse transform is X @ inv_comp.T
+        X_original = safe_sparse_dot(X, inv_comp.T, dense_output=True)
+        return X_original
+
     def _more_tags(self):
         return {
             "preserves_dtype": [np.float64, np.float32],
